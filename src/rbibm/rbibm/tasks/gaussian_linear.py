from typing import Optional, Union

import torch
from torch import Tensor
from torch.distributions import MultivariateNormal, Distribution
from rbibm.tasks.base import InferenceTask
from pyro.distributions import ConditionalDistribution


class GaussianLinearPosterior(ConditionalDistribution):
    """This computes the exact posterior for GLMs."""

    def __init__(
        self,
        prior: Distribution,
        linear_map: Tensor,
        linear_bias: Optional[Tensor] = None,
        likelihood_cov: Union[Tensor, float] = 1.0,
        eps: float = 1e-6,
    ):
        """Computes the exact posterior for a GLM.

        Args:
            prior (Distribution): Prior
            linear_map (Tensor): Linear map, given as a matrix!
            linear_bias (Tensor): Additive bias, given as a vector.
            likelihood_cov (float, Tensor): Likelihood covariance matrix, can also be a single number for diagonal.
            eps: (float): Nugget added to diagonals for numerical stability.
        """
        super().__init__()
        self.prior = prior
        if isinstance(likelihood_cov, Tensor) and likelihood_cov.ndim == 2:
            self.likelihood_cov = likelihood_cov
        elif isinstance(likelihood_cov, Tensor) and likelihood_cov.ndim == 1:
            self.likelihood_cov = torch.diag_embed(likelihood_cov)
        else:
            self.likelihood_cov = (
                torch.eye(linear_map.shape[-1]) * likelihood_cov
            )
        self.linear_map = linear_map
        self.linear_bias = linear_bias
        self.dim1 = self.linear_map.shape[0]
        self.dim2 = self.linear_map.shape[1]

        if self.linear_bias is not None and self.dim != self.linear_bias.shape[-1]:
            raise ValueError("Incompatible dimensions of linear map and bias.")

        self.eps = eps

    def condition(self, context: Tensor) -> MultivariateNormal:
        """Returns the posterior distribution

        Args:
            context (Tensor): Observations x_o.

        Returns:
            MultivariateNormal: Posterior
        """
        d = self.dim1
        d2 = self.dim2
        org_shape = context.shape
        context = context.reshape(-1, d)
        batch_shape = context.shape[0]

        prior_mean = self.prior.mean  # type: ignore
        if hasattr(self.prior, "covariance_matrix"):
            prior_cov = self.prior.covariance_matrix.to(context.device)  # type: ignore
        else:
            prior_cov = torch.eye(
                prior_mean.shape[-1], device=context.device
            ) * self.prior.variance.to(  # type: ignore
                context.device
            )

        likelihood_cov = self.likelihood_cov.to(context.device)

        gramm_matrix = self.linear_map @ prior_cov @ self.linear_map.T + likelihood_cov
        L_c = torch.linalg.cholesky(
            gramm_matrix + torch.eye(d, device=context.device) * self.eps
        )

        prior_prediction = self.linear_map @ prior_mean
        if self.linear_bias is not None:
            prior_prediction += self.linear_bias

        res = context - prior_prediction.unsqueeze(0)
        adjusted_res = torch.cholesky_solve(
            res.unsqueeze(-1), L_c.unsqueeze(0)
        ).squeeze(-1)

        post_m = prior_mean + (prior_cov @ self.linear_map.T @ adjusted_res.T).T
        post_cov = prior_cov - prior_cov @ self.linear_map.T @ torch.cholesky_solve(
            self.linear_map @ prior_cov, L_c.unsqueeze(0)
        ).repeat(batch_shape, 1, 1)

        return MultivariateNormal(
            post_m.reshape(*org_shape[:-1], d2),
            post_cov.reshape(*org_shape[:-1], d2, d2),
        )


class GeneralGaussianLinearTask(InferenceTask):
    """General gaussian linear task."""

    single_observation = True

    def __init__(
        self,
        prior_mean: Tensor,
        prior_cov: Tensor,
        linear_map: Tensor,
        likelihood_cov: Tensor,
        linear_bias: Optional[Tensor] = None,
    ) -> None:
        """States a general GLM inverse problem.

        Args:
            prior_mean (Tensor): Prior mean
            prior_cov (Tensor): Prior covariance matrix or vector of diagonal entries.
            linear_map (Tensor): Linear map
            likelihood_cov (Tensor): Likelihood covariance matrix
            linear_bias (Optional[Tensor], optional): Linear bias. Defaults to None.

        Raises:
            ValueError: If shapes do not match in the prior
            ValueError: If shapes do not match in the likelihood.
        """
        self.input_dim = linear_map.shape[0]
        self.output_dim = linear_map.shape[1]
        self.linear_map = linear_map
        self.linear_bias = linear_bias
        self.likelihood_cov = likelihood_cov

        if prior_cov.ndim == 2:
            prior = torch.distributions.MultivariateNormal(prior_mean, prior_cov)
        elif prior_cov.ndim == 1:
            prior = torch.distributions.Independent(
                torch.distributions.Normal(prior_mean, prior_cov), 1
            )
        else:
            raise ValueError("The prior cov must be either a matrix or a vector...")

        if likelihood_cov.ndim == 2:

            def likelihood_fn(theta):  # type: ignore
                weight = self.linear_map.to(theta.device)
                bias = self.linear_bias
                if bias is not None:
                    bias = bias.to(theta.device)
                mean = torch.nn.functional.linear(theta, weight, bias)
                return MultivariateNormal(mean, self.likelihood_cov)

        elif likelihood_cov.ndim == 1:
            self.likelihood_scale = self.likelihood_cov.sqrt()

            def likelihood_fn(theta):
                weight = self.linear_map.to(theta.device)
                bias = self.linear_bias
                if bias is not None:
                    bias = bias.to(theta.device)

                mean = torch.nn.functional.linear(theta, weight, bias)
                return torch.distributions.Independent(
                    torch.distributions.Normal(
                        mean, self.likelihood_scale.to(theta.device)
                    ),
                    1,
                )

        else:
            raise ValueError(
                "The likelihood cov must be either a matrix or a vector..."
            )

        super().__init__(prior, likelihood_fn, None)

    def get_true_posterior(self, device="cpu"):
        return GaussianLinearPosterior(
            prior=self.get_prior(device=device),
            linear_map=self.linear_map.to(device),
            likelihood_cov=(self.likelihood_cov).to(device),
        )


class GaussianLinearTask(GeneralGaussianLinearTask):

    """Simple gaussian linear task, where the parameters are transformed by a diagonal matrix."""

    single_observation = True

    def __init__(
        self,
        dim: int = 10,
        prior_mean: float = 0.0,
        prior_scale: float = 1.0,
        likelihood_scale: float = 0.1,
    ):
        """Simple gaussian linear task, were the parameters are transformed by a diagonal matrix.

        Args:
            dim (int, optional): Dimension of task. Defaults to 10.
            prior_mean (float, optional): Prior mean. Defaults to 0.0.
            prior_scale (float, optional): Prior standard deviation. Defaults to 1.0.
            likelihood_scale (float, optional): Likelihood standard deviation. Defaults to 0.1.
        """
        mean = prior_mean * torch.ones(dim)
        cov = prior_scale**2 * torch.ones(dim)
        likelihood_cov = likelihood_scale**2 * torch.ones(dim)
        linear_map = torch.diag_embed(_A_diag[:dim])

        super().__init__(
            prior_mean=mean,
            prior_cov=cov,
            likelihood_cov=likelihood_cov,
            linear_map=linear_map,
        )
        

class HardGaussianLinearTask(GeneralGaussianLinearTask):
    def __init__(
        self,
        dim1: int = 100,
        dim2: int = 10,
        prior_mean: float = 0.0,
        prior_scale: float = 1.0,
        seed: float = 0.,
    ):
        """Hard gaussian linear task, were the parameters are transformed by a diagonal matrix.

        Args:
            dim (int, optional): Dimension of task. Defaults to 10.
            prior_mean (float, optional): Prior mean. Defaults to 0.0.
            prior_scale (float, optional): Prior standard deviation. Defaults to 1.0.
            likelihood_scale (float, optional): Likelihood standard deviation. Defaults to 0.1.
        """
        torch.manual_seed(seed)
        mean = prior_mean * torch.ones(dim2)
        cov = prior_scale**2 * torch.ones(dim2)
        scale = torch.eye(dim1)*1.5#+ torch.randn(dim1, dim1)*0.1
        likelihood_cov = scale@scale.T
        linear_map = torch.randn(dim1, dim2)*2 -1
        linear_bias = torch.randn(dim1)*0.5


        super().__init__(
            prior_mean=mean,
            prior_cov=cov,
            likelihood_cov=likelihood_cov,
            linear_map=linear_map,
            linear_bias=linear_bias,
        )


# Constants

_A_diag = torch.tensor(
    [
        -2.6330e-01,
        -1.9402e00,
        1.6822e00,
        -2.9684e-01,
        -3.1292e-02,
        -1.0250e00,
        5.6811e-02,
        2.9649e-01,
        1.2073e00,
        -1.7704e00,
        -5.0733e-01,
        8.7393e-01,
        -5.6353e-01,
        -3.5100e-01,
        -9.4086e-02,
        1.1572e00,
        8.9916e-01,
        -6.0620e-01,
        4.9354e-01,
        2.3987e-01,
        7.5735e-01,
        -6.4855e-01,
        -2.8359e-01,
        -1.9448e00,
        7.0825e-02,
        -8.1082e-01,
        4.0365e-01,
        -1.8183e00,
        -1.3453e00,
        -7.5972e-01,
        -5.1666e-01,
        -1.7230e00,
        -2.3211e-01,
        6.6439e-01,
        -1.9995e00,
        -1.8906e00,
        -2.6568e00,
        6.9307e-01,
        2.7303e-01,
        1.3710e-01,
        4.9097e-01,
        8.1474e-02,
        8.3232e-01,
        2.1458e00,
        -1.8058e00,
        -1.3288e00,
        -7.5003e-01,
        1.0397e00,
        -2.1856e00,
        -9.8418e-01,
        1.0465e00,
        1.7250e-01,
        -9.1463e-01,
        -2.4919e-01,
        1.2401e00,
        1.0118e-01,
        -1.8794e-01,
        -5.0070e-01,
        4.2194e-01,
        -5.0441e-01,
        -6.1415e-01,
        -5.3107e-03,
        5.1072e-01,
        3.2868e-01,
        -6.6389e-01,
        1.4012e-01,
        -5.4285e-01,
        -6.5971e-01,
        2.0715e-01,
        -1.8451e00,
        -6.9357e-01,
        -2.0454e-01,
        4.5503e-01,
        -1.5295e00,
        9.8413e-01,
        8.6398e-01,
        3.6432e-01,
        -5.6952e-01,
        2.8906e-01,
        9.9933e-01,
        -1.9779e00,
        -2.1395e-01,
        2.9220e00,
        -7.7481e-01,
        -6.0019e-01,
        -9.7341e-01,
        -4.5467e-01,
        7.8136e-01,
        1.4214e00,
        -3.3630e-01,
        4.1162e-01,
        -1.2698e00,
        -4.0045e-01,
        4.6323e-03,
        -1.0570e00,
        -5.2240e-01,
        -3.1386e-01,
        7.2385e-01,
        1.3401e00,
        -1.8672e-01,
        1.5349e-01,
        -5.1670e-01,
        8.6386e-01,
        -8.7582e-01,
        -5.1301e-01,
        -8.8537e-01,
        -2.2805e00,
        -1.3339e00,
        1.6169e00,
        3.3533e-01,
        -3.5906e-01,
        1.9146e00,
        4.7599e-01,
        -5.5192e-02,
        -1.5592e-01,
        -1.6221e-01,
        -9.4099e-01,
        5.2955e-01,
        -4.5892e-01,
        -5.9098e-02,
        -4.2485e-01,
        8.7224e-01,
        6.1242e-02,
        -1.8527e00,
        -3.2166e-01,
        -5.1652e-01,
        3.0804e-01,
        -3.6569e-01,
        -1.3729e00,
        2.4304e-01,
        1.0243e00,
        -2.3333e00,
        9.7887e-01,
        -9.8544e-01,
        -1.0509e00,
        -1.5113e00,
        1.1724e00,
        2.7618e-01,
        8.7501e-01,
        -1.8123e-01,
        3.7264e-01,
        1.0852e00,
        -1.5609e00,
        8.0923e-01,
        -1.8558e-01,
        4.4373e-01,
        -8.6303e-01,
        4.7194e-01,
        3.7066e00,
        -2.6731e-01,
        5.8528e-01,
        1.6402e00,
        -4.7821e-01,
        3.0227e-01,
        2.4146e-01,
        2.7620e-02,
        -4.6235e-01,
        3.9010e-01,
        9.5593e-01,
        -1.0287e-01,
        -1.0578e00,
        -1.2663e-01,
        -1.4609e00,
        3.3759e-01,
        -6.5913e-01,
        -1.9012e00,
        -7.4355e-01,
        1.1583e-01,
        -1.0010e00,
        -1.6898e00,
        -2.7932e-01,
        8.8420e-01,
        2.5372e-01,
        2.4825e-01,
        4.5060e-01,
        1.0094e00,
        -7.8274e-01,
        -5.3718e-01,
        -9.6005e-02,
        5.9447e-01,
        8.3396e-01,
        -1.5999e00,
        1.7461e00,
        2.0917e00,
        -3.4235e-01,
        2.7156e00,
        2.2899e-01,
        -3.9593e-01,
        -5.9732e-01,
        1.7926e00,
        1.1241e00,
        1.0419e-01,
        4.0984e-01,
        7.7669e-01,
        9.8019e-01,
        8.9707e-01,
        -7.8094e-03,
        7.0142e-01,
        6.0455e-01,
        -1.0457e00,
        -5.4153e-01,
        4.0958e-01,
        -1.2435e00,
        9.6165e-02,
        -7.6112e-01,
        1.5654e00,
        2.3165e-01,
        -5.9158e-01,
        -2.3454e-01,
        6.2039e-02,
        4.7744e-01,
        -2.2735e-01,
        2.7178e-01,
        -1.1905e00,
        9.3933e-01,
        4.5101e-01,
        -2.2661e00,
        -2.0420e00,
        7.3868e-01,
        3.7871e-01,
        6.2175e-01,
        7.2030e-02,
        2.8397e-01,
        -2.7589e-02,
        -2.9781e-01,
        -1.2231e00,
        3.3085e-01,
        9.5001e-02,
        -1.4395e00,
        -1.8053e00,
        4.0120e-01,
        7.1553e-01,
        -2.2649e00,
        -1.2014e00,
        1.0363e-02,
        -7.4623e-01,
        -2.5301e-01,
        -8.2052e-01,
        7.5799e-02,
        3.9808e-01,
        -5.5368e-01,
        5.2935e-01,
        5.1309e-02,
        1.2339e00,
        2.9099e00,
        -1.2287e00,
        -8.4154e-01,
        -1.8261e00,
        -4.7483e-01,
        -5.4979e-01,
        2.0432e00,
        8.3078e-01,
        1.1432e-02,
        -7.6006e-01,
        -4.6215e-01,
        2.7904e00,
        -8.2975e-02,
        -1.2283e00,
        8.5189e-01,
        3.1508e-01,
        2.7507e-01,
        4.2855e-01,
        3.0178e-02,
        -1.7786e-01,
        7.6557e-01,
        2.7636e-01,
        -5.1843e-01,
        2.5476e-01,
        -5.7780e-01,
        1.3537e00,
        -1.0424e-01,
        1.5311e00,
        1.5767e00,
        3.3789e-01,
        2.1926e-02,
        7.2589e-01,
        -6.8083e-01,
        1.1217e-01,
        -6.7519e-01,
        1.5569e00,
        -1.0290e00,
        -1.2899e00,
        1.3841e00,
        -7.4224e-01,
        -3.0036e-01,
        2.2587e-01,
        6.1922e-01,
        6.1602e-01,
        8.3257e-01,
        -7.9445e-01,
        -1.0803e00,
        -1.5686e-01,
        -8.3703e-01,
        2.4196e-01,
        -1.2277e00,
        -1.0956e00,
        1.7853e00,
        1.8202e00,
        -4.3102e-01,
        -1.9885e00,
        9.3865e-01,
        4.2489e-01,
        -1.1037e00,
        1.9769e-02,
        -8.9176e-01,
        3.7860e-01,
        4.3754e-01,
        7.5574e-01,
        -1.2009e00,
        -8.3057e-01,
        2.9801e-01,
        4.4374e-03,
        1.2174e00,
        -9.0078e-01,
        -1.1663e-01,
        4.0228e-02,
        -9.8264e-01,
        2.2644e00,
        -1.7315e00,
        -1.0695e-01,
        -2.9181e-01,
        1.5623e00,
        1.4722e00,
        2.1043e-01,
        -6.7115e-01,
        9.9438e-02,
        7.3665e-01,
        1.1439e00,
        5.7616e-01,
        1.7145e00,
        2.1169e-01,
        -2.1103e00,
        -3.3170e-01,
        -8.6573e-01,
        1.5072e-02,
        -5.8795e-01,
        -2.3847e-01,
        1.2449e00,
        1.0779e00,
        4.4117e-01,
        3.6250e-01,
        -7.2643e-01,
        -1.0670e-02,
        1.0787e00,
        -5.5335e-01,
        2.8696e-01,
        1.0590e00,
        1.7062e-01,
        -1.6668e-01,
        1.8390e00,
        -3.3607e-02,
        -7.0391e-01,
        -2.3788e-01,
        -5.1033e-01,
        6.6370e-01,
        -4.1219e-01,
        6.0117e-01,
        -8.6290e-01,
        2.0941e-01,
        1.0266e00,
        -1.7119e00,
        -9.2337e-01,
        -1.8797e00,
        -3.4894e-01,
        -1.7510e00,
        5.4410e-01,
        4.6059e-01,
        2.1924e-01,
        -6.2130e-01,
        1.6387e-01,
        -7.2150e-01,
        -5.4066e-01,
        4.2456e-01,
        2.7741e-01,
        -3.1399e-01,
        1.2940e00,
        -3.8458e-01,
        5.4730e-01,
        3.9456e00,
        -4.3822e-01,
        1.2891e00,
        1.4031e00,
        4.3756e-01,
        7.1864e-01,
        5.9924e-01,
        5.7325e-01,
        -4.4882e-01,
        9.6156e-01,
        3.7345e-01,
        -1.4583e00,
        -5.8776e-01,
        1.5822e00,
        -9.0913e-01,
        -1.4096e00,
        5.8571e-01,
        -1.1057e00,
        8.6691e-02,
        4.6549e-01,
        9.1266e-01,
        1.4929e-01,
        4.2129e-01,
        1.8826e-01,
        -1.0468e00,
        -8.2832e-01,
        4.1022e-01,
        -6.7697e-01,
        -6.5440e-01,
        -1.4227e-01,
        5.1594e-01,
        -3.4314e-01,
        -1.2036e00,
        3.7619e-03,
        -1.3309e-02,
        -1.7393e00,
        -9.1883e-01,
        -3.8793e-01,
        4.3591e-01,
        7.7753e-02,
        8.4050e-01,
        1.7332e00,
        7.0941e-01,
        5.8034e-01,
        8.5878e-01,
        5.6435e-02,
        1.2197e00,
        -1.4879e00,
        7.6300e-01,
        -4.5594e-01,
        -8.4862e-01,
        -1.4286e00,
        6.4136e-01,
        -3.7048e-02,
        -2.3096e-01,
        -1.3013e-01,
        5.6234e-01,
        8.2614e-01,
        -7.2783e-01,
        -3.4298e-01,
        -8.7518e-01,
        -9.8312e-01,
        -1.3974e00,
        1.9388e-01,
        -1.1281e00,
        1.2626e00,
        -1.3978e-01,
        -7.8992e-01,
        6.1852e-01,
        -1.0356e00,
        9.5489e-01,
        -5.5253e-01,
        -5.9567e-01,
        8.6033e-01,
        -2.3549e-01,
        3.3985e-01,
        -1.1379e00,
        -3.4728e-01,
        -1.4779e00,
        -3.4322e-02,
        -1.1557e00,
        -2.4303e00,
        1.4901e00,
        -1.3934e00,
        -8.4631e-02,
        1.2406e00,
        -4.2637e-01,
        6.1287e-01,
        5.3378e-01,
        1.0367e00,
        4.1378e-01,
        1.7605e00,
        -6.3472e-01,
        -7.7975e-01,
        8.4842e-01,
        -6.4412e-01,
        -5.2813e-01,
        -2.5058e00,
        2.4183e00,
        1.7048e00,
        -2.3734e00,
        2.0195e00,
        6.7951e-01,
        -1.4071e00,
        -2.5009e-02,
        3.3783e-01,
        -1.2525e00,
        4.9407e-01,
        1.0406e00,
        -1.2392e-01,
        -4.8565e-01,
        7.0423e-01,
        -1.8660e00,
        -7.9736e-01,
        -4.7907e-01,
        1.2006e-01,
        9.6424e-01,
        -8.8924e-01,
        5.1609e-01,
        -1.1653e00,
        1.0349e00,
        -5.2150e-01,
        -3.2552e-01,
        -1.6618e00,
        1.1421e00,
        1.1851e00,
        -8.2986e-02,
        -3.4383e-01,
        1.2509e00,
        8.9787e-02,
        -2.8505e-01,
        -1.4728e00,
        6.5363e-02,
        -1.9194e00,
        -1.1687e-01,
        4.3301e-01,
        -7.0487e-01,
        3.8271e-01,
        1.5245e00,
        -1.6484e-01,
        4.5764e-01,
        1.0948e00,
        -5.6648e-02,
        4.8381e-01,
        1.6642e-01,
        -2.3124e-01,
        1.5345e-01,
        3.6960e-01,
        -1.0992e-01,
        7.1653e-01,
        1.8405e00,
        -1.5964e00,
        5.1450e-02,
        -2.0910e-01,
        1.6182e-01,
        5.8876e-01,
        -7.7056e-01,
        -6.9967e-02,
        -1.1238e-01,
        1.6658e00,
        -1.7180e-02,
        -1.0518e00,
        -1.1156e00,
        -4.0438e-01,
        1.4184e00,
        -4.0826e-02,
        -1.2735e00,
        -5.0961e-01,
        1.6033e00,
        1.4176e00,
        -1.1196e-01,
        3.0745e-01,
        1.1659e00,
        1.7934e00,
        1.3227e00,
        -9.2205e-01,
        -1.3787e00,
        -9.9151e-01,
        6.2656e-01,
        1.1656e00,
        -8.2119e-01,
        1.0637e00,
        1.5134e00,
        1.0484e00,
        -2.4260e-01,
        -2.7637e00,
        2.1025e-01,
        -5.0735e-01,
        1.2567e00,
        -5.5368e-01,
        -1.6725e00,
        7.0268e-01,
        1.1251e00,
        3.9760e-01,
        4.2253e-01,
        5.6641e-01,
        5.1135e-01,
        7.0528e-02,
        8.2743e-01,
        -5.7939e-01,
        -4.0445e-01,
        2.4095e-02,
        -1.3684e00,
        5.4827e-01,
        -2.8018e-01,
        -3.6398e-01,
        -5.8772e-01,
        -4.5432e-01,
        4.8648e-01,
        -7.0395e-01,
        1.0721e00,
        -1.1145e00,
        1.4843e-02,
        -1.7606e00,
        -1.3532e00,
        -5.6324e-01,
        1.1828e00,
        -1.2093e00,
        3.6442e00,
        -1.9316e-01,
        -1.5457e00,
        2.1623e00,
        -5.2938e-01,
        -8.3875e-01,
        4.4452e-01,
        -1.3509e00,
        -1.6986e00,
        -3.4147e-01,
        4.7070e-01,
        -5.8553e-01,
        4.8729e-01,
        -2.2173e-01,
        2.1870e00,
        7.9101e-01,
        4.5007e-01,
        -3.4048e-01,
        6.8266e-01,
        3.9330e-01,
        8.0064e-01,
        -1.5266e-01,
        9.1356e-01,
        2.0143e-01,
        4.1783e-01,
        6.2648e-02,
        9.4921e-01,
        -1.7954e-01,
        1.6919e00,
        -3.9969e-02,
        -3.2538e-01,
        1.6151e00,
        1.7300e00,
        5.0825e-01,
        4.8681e-02,
        -7.1221e-01,
        -1.1640e00,
        3.4546e-01,
        7.5224e-01,
        2.5560e-01,
        -2.2374e-02,
        -9.0706e-01,
        -5.8510e-01,
        4.8371e-01,
        2.1125e00,
        -1.4310e00,
        5.6866e-02,
        -1.7116e00,
        -3.0661e-01,
        1.7142e00,
        -9.1236e-01,
        -7.5986e-01,
        9.4115e-03,
        -1.8236e-01,
        9.0294e-01,
        6.5358e-01,
        -1.1342e00,
        -7.1613e-01,
        -2.4787e-01,
        1.0769e00,
        2.0874e00,
        -3.2278e-02,
        -6.2902e-01,
        1.1513e00,
        1.2819e00,
        -1.7410e00,
        1.1033e00,
        3.0529e-01,
        1.7497e-01,
        8.5064e-01,
        -4.9799e-01,
        -1.5327e00,
        -4.3670e-01,
        -4.4812e-01,
        2.5661e00,
        1.3481e00,
        6.0113e-01,
        3.2461e-01,
        6.4277e-01,
        -1.2109e-01,
        -1.2738e00,
        -1.0217e00,
        1.0488e-01,
        7.2125e-01,
        1.3882e00,
        1.7869e00,
        7.6932e-02,
        -2.4757e-01,
        1.6362e00,
        -6.1692e-01,
        -1.2083e00,
        -1.4070e00,
        1.3728e-01,
        -9.8060e-01,
        -3.9617e-01,
        3.1414e-01,
        4.4218e-02,
        -1.2126e00,
        -6.9145e-01,
        1.0879e00,
        -6.6560e-01,
        -1.2797e00,
        -5.5597e-01,
        1.1442e00,
        -1.4750e00,
        -1.7438e00,
        -1.4212e00,
        5.5803e-01,
        -1.5973e00,
        -6.8830e-02,
        2.3635e-01,
        6.8890e-01,
        7.7617e-01,
        3.1402e-01,
        3.7189e-01,
        1.3374e-01,
        -7.5310e-01,
        1.9480e-01,
        1.6522e00,
        4.6527e-01,
        2.8774e00,
        3.0583e-01,
        4.8665e-01,
        1.0862e00,
        3.3559e-01,
        1.0196e-01,
        -6.2857e-01,
        -8.7931e-01,
        3.7650e-01,
        -7.8459e-01,
        -9.0455e-02,
        -2.4926e-01,
        -6.5317e-01,
        -4.4407e-01,
        1.3480e-01,
        7.6954e-01,
        -1.2496e00,
        1.9966e00,
        1.2608e00,
        6.5469e-01,
        2.1547e-01,
        3.1031e-01,
        -1.8016e00,
        -1.4201e00,
        -1.3209e00,
        -7.4082e-02,
        7.3979e-02,
        -4.0351e-01,
        6.7084e-02,
        5.8197e-01,
        1.5977e00,
        1.6111e00,
        -4.2631e-01,
        -1.0443e-01,
        -2.7987e-01,
        5.7299e-01,
        -1.1804e00,
        4.2756e-01,
        2.6845e-01,
        1.5483e-01,
        -4.8244e-01,
        -1.1794e00,
        1.0882e00,
        -7.2757e-02,
        2.0332e00,
        -4.6975e-01,
        9.7551e-03,
        1.0090e00,
        1.5725e00,
        -1.0120e-01,
        9.0597e-02,
        -6.8937e-02,
        -7.4270e-01,
        2.6081e-01,
        -6.1635e-01,
        1.5183e00,
        -7.0694e-01,
        5.6431e-01,
        -8.1275e-01,
        -2.5181e-01,
        -1.0590e00,
        -2.7656e-02,
        -7.2458e-01,
        -5.1309e-01,
        4.2757e-01,
        1.2121e00,
        -1.8625e-01,
        7.0136e-01,
        -1.9744e-01,
        1.2951e-01,
        -8.5800e-01,
        -2.5035e-01,
        1.8913e00,
        -2.9256e-01,
        1.6072e00,
        5.5937e-01,
        -1.2268e00,
        -1.3633e00,
        1.7203e00,
        -1.9616e00,
        -3.6385e-01,
        4.2951e-01,
        5.9268e-01,
        -2.2327e00,
        3.6666e-01,
        2.0490e-01,
        -1.5722e00,
        -3.2426e00,
        1.0397e-01,
        5.9987e-01,
        9.2798e-02,
        1.0146e-01,
        -1.0622e00,
        3.5234e-01,
        -9.3607e-01,
        7.5352e-03,
        -1.8177e-01,
        -5.5925e-01,
        5.2180e-01,
        1.5204e00,
        1.2351e00,
        9.8539e-01,
        -1.2978e00,
        -1.7945e00,
        1.4231e00,
        -5.6741e-01,
        2.7732e00,
        -1.5786e00,
        2.2712e-01,
        -5.7977e-01,
        -3.1850e-01,
        4.5839e-01,
        8.2838e-01,
        1.2952e00,
        -3.6497e-01,
        6.5351e-01,
        2.0357e00,
        8.3237e-01,
        1.2954e00,
        1.9905e-01,
        8.2659e-01,
        2.6167e-01,
        -9.9828e-01,
        2.1760e-01,
        1.6275e-02,
        -5.4546e-01,
        1.0532e00,
        -1.5201e00,
        -5.4651e-01,
        1.6475e-01,
        -5.4067e-01,
        -1.2567e00,
        -1.3868e-01,
        -6.7944e-02,
        -4.6458e-02,
        1.1140e00,
        9.1979e-01,
        -4.3165e-01,
        -1.5229e00,
        -6.6485e-01,
        3.0222e-01,
        3.7211e-01,
        -1.1501e00,
        -1.5597e00,
        3.6514e-01,
        1.9342e-01,
        -1.3579e00,
        -1.4513e00,
        -1.5150e00,
        5.5862e-01,
        -2.1725e-01,
        -3.1463e-01,
        -3.9517e-01,
        1.1580e-01,
        8.4483e-01,
        1.1180e00,
        9.0287e-01,
        1.5690e00,
        1.2631e00,
        1.0039e00,
        8.8854e-02,
        -1.0078e00,
        1.6163e00,
        -5.4192e-01,
        -8.4952e-01,
        -4.1470e-01,
        -4.7238e-01,
        -1.3277e00,
        -9.9609e-01,
        1.0442e00,
        -9.2275e-01,
        -2.8962e-02,
        -1.3889e-01,
        -1.7369e00,
        -1.7446e-01,
        5.1822e-01,
        -1.2528e00,
        8.6472e-01,
        -7.9484e-01,
        1.0268e00,
        -5.7840e-01,
        9.4688e-01,
        -3.5200e-01,
        6.7091e-01,
        -5.6271e-01,
        -7.0368e-01,
        4.4811e-01,
        -1.1883e-01,
        1.8801e-02,
        1.8860e00,
        -1.2569e00,
        -4.4744e-01,
        6.8385e-01,
        8.9061e-02,
        -2.8686e-01,
        8.5032e-01,
        5.2401e-01,
        6.7523e-01,
        2.8595e-01,
        -1.8831e00,
        1.1234e00,
        1.0491e-01,
        5.4410e-01,
        5.3977e-01,
        -5.6934e-01,
        -3.6979e-01,
        -1.6456e00,
        -4.5295e-01,
        1.3816e00,
        -2.8919e-01,
        4.3207e-01,
        1.0801e00,
        -1.1129e-01,
        -7.8578e-02,
        5.4604e-01,
        -2.3807e-01,
        -2.8595e-01,
        -2.9758e-01,
        7.0888e-01,
        4.7438e-01,
        -4.1297e-01,
        -1.1932e00,
        4.2952e-01,
        3.1907e-01,
        1.4105e00,
        4.2227e-01,
        1.0413e-01,
        3.0342e-01,
        7.0236e-01,
        1.1387e00,
        -9.4426e-01,
        -4.7201e-03,
        3.2996e-01,
        3.6056e-01,
        5.0060e-01,
        -4.6886e-01,
        -1.4661e00,
        2.6997e-01,
        -1.4341e-01,
        8.5637e-01,
        -1.7491e-01,
        -4.0830e-01,
        6.8186e-01,
        -4.7209e-01,
        -4.7600e-01,
        -2.4721e00,
        1.3887e-01,
        -3.0429e-01,
        -1.5111e00,
        4.2217e-01,
        -6.9243e-01,
        1.3122e00,
        7.3534e-02,
        -1.7099e00,
        1.2675e00,
        2.2677e00,
        -2.8945e-01,
        1.9415e-01,
        1.1300e00,
        -3.0945e00,
        -4.6790e-01,
        -5.5702e-02,
        1.7721e00,
        -2.4404e-01,
        1.1408e00,
        9.8379e-01,
        -1.1874e-01,
        -4.0909e-01,
        -3.8042e-01,
        -2.8415e-01,
        -5.5763e-01,
        8.1921e-01,
        -2.1972e-01,
        -1.5643e-01,
        2.2570e-01,
        -6.6557e-01,
        -1.9675e-01,
        6.7165e-01,
        -2.2928e-01,
        1.6263e00,
        1.5781e-01,
        -1.6282e00,
        -1.7994e00,
        1.7299e-01,
    ]
)
