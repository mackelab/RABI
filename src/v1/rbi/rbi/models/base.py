import torch
from torch import nn, Tensor

from torch.distributions import Distribution, Transform, Normal

from typing import Union, Callable, Optional, List, Tuple, Any

import sbi.inference.posteriors
from sbi.samplers.vi.vi_utils import move_all_tensor_to_device

from pyro.distributions.transforms import permute, Permute, IndependentTransform

from rbi.utils.distributions import ConditionalTransformedDistribution


class Generator:
    """This serves as base class for mappings that input a set of parameters and returns a probability distribution"""

    def __init__(self, d: int) -> None:
        """Dimension of paramteriztaion.

        Args:
            d (int): Dimension of paramters.
        """
        self.d = d

    def generate_distribution(self, phi: Tensor) -> Distribution:
        """Inputs a set of free parameters phi and outputs a probability distributions generated from this parameters.

        Args:
            phi (Tensor): Free parameters

        Raises:
            NotImplementedError: Must be implemented

        Returns:
            Distribution: Distribution parameterized by phi.
        """
        raise NotImplementedError("Not implemented")

    def __call__(self, phi) -> Distribution:
        return self.generate_distribution(phi)


class GeneratorWithTransform:
    """Applies a transformation to the distribution obtained through a generator."""

    def __init__(self, generator: Union[Generator, Callable], transform: Transform):
        self.generator = generator
        self.transform = transform

    def __call__(self, phi: Tensor) -> Distribution:
        p = self.generator(phi)
        # if isinstance(p, torch.distributions.Independent):
        #     return torch.distributions.Independent(
        #         torch.distributions.TransformedDistribution(
        #             p.base_dist, self.transform
        #         ),
        #         1,
        #     )
        # else:
        return torch.distributions.TransformedDistribution(p, self.transform)


class ParametricProbabilisticModel(nn.Module):

    """This implements a mapping into the space of probability distributions of a certain parametric family i.e. Normals."""

    def __init__(
        self,
        net: nn.Module,
        generator: Union[Generator, Callable],
        embedding_net: nn.Module = nn.Identity(),
        output_transform: Optional[Transform] = None,
        prediction_fn: Union[str, Callable] = "sample",
    ) -> None:
        """Implements a mapping into from inputs x to the space of a certain parametric family of distributions as generated by the 'generator' through a neural net mapping onto the free paratmers.

        Args:
            net (nn.Module): Neural network mapping returning free parameters of a probability distribution.
            generator (Union[Generator, Callable]): Generator gets the parameters and outputs a distribution.
            embedding_net (nn.Module, optional): An embedding net for learnable summary statistics. Defaults to nn.Identity().
            output_transform (Optional[Transform], optional): Output transform to map onto a certain support. Defaults to None.
            prediction_fn (Union[str, Callable], optional): Prediction function. Defaults to "sample".
        """

        super().__init__()
        self.net = net
        self.embedding_net = embedding_net
        self.output_transform = output_transform

        if self.output_transform is not None:
            self.generator = GeneratorWithTransform(generator, self.output_transform)
        else:
            self.generator = generator

        self._set_prediction_fn(prediction_fn)

    def _predict_mean(self, x: Tensor) -> Tensor:
        """Return the mean of the predicted distribution"""
        return self.forward(x).mean

    def _predict_mean_std(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """Return the mean and standard deviation of the predicted distribution"""
        p = self.forward(x)
        return p.mean, p.stddev

    def _prediction_sample(self, x: Tensor) -> Tensor:
        """Return a sample from the predicted distribution"""
        p = self.forward(x)
        if p.has_rsample:
            return p.rsample()
        else:
            return p.sample()

    def _prediction_argmax(self, x: Tensor) -> Tensor:
        """Return the argmax of the predicted distribution"""
        return self.forward_parameters(x).round()

    def to(self, device: str) -> "ParametricProbabilisticModel":
        """Moves all necessary tensors to the right device"""
        super().to(device)
        move_all_tensor_to_device(self.output_transform, device)
        return self

    @property
    def prediction_fn(self) -> Callable:
        """Return the current prediction function"""
        return self._prediction_fn

    @prediction_fn.setter
    def prediction_fn(self, fn: Union[Callable, str]) -> None:
        """Sets the prediction function

        Args:
            fn (Union[Callable,str]): We support mean, mean_std, sample, dist and argmax as default. But you can also directly pass a function.
        """
        self._set_prediction_fn(fn)

    def _set_prediction_fn(self, fn: Union[str, Callable]) -> None:
        if fn == "argmax":
            self._prediction_fn = self._prediction_argmax
        elif fn == "sample":
            self._prediction_fn = self._prediction_sample
        elif fn == "dist":
            self._prediction_fn = self.forward
        elif fn == "mean":
            self._prediction_fn = self._predict_mean
        elif fn == "mean_std":
            self._prediction_fn = self._predict_mean_std
        else:
            if isinstance(fn, Callable):
                self._prediction_fn = fn
            else:
                ValueError(
                    "Invalid value, please provide an Callable or on of 'argmax' or 'sample' as arguments."
                )

    def forward_parameters(self, x: Tensor) -> Tensor:
        """Pushes the inputs through the neural net and return the parameters.

        Args:
            x (Tensor): Inputs

        Returns:
            Tensor: Output parameters.
        """
        if x.shape == torch.Size([]):
            x = x.reshape(1)

        x = self.embedding_net(x)
        return self.net(x)

    def forward(self, x: Tensor) -> Distribution:
        """Input a tensor and outputs a probability distribution.

        Args:
            x (Tensor): Inputs

        Returns:
            Distribution: Output distribution
        """
        if x.shape == torch.Size([]):
            x = x.reshape(1)

        params = self.forward_parameters(x)
        self._params_cached = params.clone().detach()
        return self.generator(params)

    def predict(self, x: Tensor) -> Any:
        """Predicts the outcome, depending on which

        Args:
            x (Tensor): Input x on which prediction are evaluated
        """
        return self._prediction_fn(x)


class ZukoFlowModel(nn.Module):
    def __init__(
        self,
        flow_class: type,
        input_dim: int,
        output_dim: int,
        hidden_dims: List[int] = [50, 50],
        num_transforms: int = 5,
        randperm: bool = True,
        output_transform: Optional[Transform] = None,
        embedding_net: Optional[nn.Module] = None,
        **kwargs,
    ) -> None:
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        if embedding_net is not None:
            self.embedding_net = embedding_net
        else:
            self.embedding_net = nn.Identity()
        self.output_transform = output_transform
        self._kwargs = kwargs
        

        context_dim = self.embedding_net(torch.zeros((1,input_dim))).shape[-1]
        self.flow = flow_class(features=self.output_dim, context=context_dim, hidden_features=hidden_dims, transforms=num_transforms, randperm=randperm, **kwargs)


    def forward_embedding(self, x: Tensor) -> Tensor:
        """Forward the embedded inputs i.e. the learned summary statistics.

        Args:
            x (Tensor): Inputs

        Returns:
            Tensor: Embedding
        """
        return self.embedding_net(x)

    def forward(self, x: Tensor) -> Distribution:
            """Returns a distribution given an input.

            Args:
                x (Tensor): Input

            Returns:
                Distribution: Output distribution
            """
            q = self.flow(self.embedding_net(x))
            if self.output_transform is not None:
                q.transforms += [self.output_transform]

            return q

    def to(self, device: str):
        self.flow.to(device)
        move_all_tensor_to_device(self.output_transform, device)
        return self




class PyroFlowModel(nn.Module):
    """Conditional normalizing flow based on pyro."""

    def __init__(
        self,
        transform_generator: Callable,
        input_dim: int,
        output_dim: int,
        hidden_dims: List[int] = [50, 50],
        num_transforms: int = 5,
        randperm: bool = True,
        with_cache: bool = True,
        output_transform: Optional[Transform] = None,
        embedding_net: Optional[nn.Module] = None,
        **kwargs
    ):
        """Conditional normalizing flow based on pyro.

        Args:
            transform_generator (Callable): Function that generates a learnable transformation.
            input_dim (int): Input dimension
            output_dim (int): Output dimension
            hidden_dims (List[int], optional): Hidden dims of nns used in learnable transformations . Defaults to [50, 50].
            num_transforms (int, optional): Number of transformations. Defaults to 5.
            shuffle (bool, optional): Permuting dimensions, good for autoregressive or coupling flows. Defaults to True.
            with_cache (bool, optional): If intermediate results should be cached to speed up computation. Defaults to False.
            output_transform (Optional[Transform], optional): Output transform. Defaults to None.
            embedding_net (Optional[nn.Module], optional): Embedding net. Defaults to None.
        """
        super().__init__()
        if embedding_net is not None:
            self.embedding_net = embedding_net
        else:
            self.embedding_net = nn.Identity()

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.with_cache = with_cache
        self._kwargs = kwargs

        input_dim = self.embedding_net(torch.zeros((1,input_dim))).shape[-1]

        if not hasattr(self, "transforms"):
            self.transforms = []

        for i in range(num_transforms):
            transform = transform_generator(
                output_dim, input_dim, hidden_dims=hidden_dims, **kwargs
            )

            self.register_module("t" + str(i), transform)
            self.transforms.append(transform)
            self._modules["t" + str(i)] = transform
            if randperm:
                permut_layer = permute(output_dim)
                self.register_buffer("permute" + str(i), permut_layer.permutation)
                self.register_buffer(
                    "inv_permute" + str(i), permut_layer.inv_permutation  # type: ignore
                )
                self.transforms.append(permut_layer)

        if output_transform is not None:
            self.transforms.append(output_transform)

        self.set_base_dist()

        self.distribution = ConditionalTransformedDistribution(
            self.base_dist, self.transforms
        )

    def set_base_dist(self):
        """Sets the base distribution of the flow to an standard normal distribution."""
        self.register_buffer("base_dist_mean", torch.zeros(self.output_dim))
        self.register_buffer("base_dist_scale", torch.ones(self.output_dim))
        self.base_dist = Normal(
            self.base_dist_mean, self.base_dist_scale, validate_args=False
        )

        # To keep the device up to date...
        self.base_dist.loc = self.base_dist_mean
        self.base_dist.scale = self.base_dist_scale

    def to(self, device: str) -> nn.Module:
        """Moves all tensors to the right device

        Args:
            device (str): Device to use

        Returns:
            nn.Module: self
        """
        super().to(device)
        self.base_dist.loc = self.base_dist_mean
        self.base_dist.scale = self.base_dist_scale

        # Non module transforms...
        i = 0
        for t in self.transforms:
            if isinstance(t, Permute):
                t.permutation = getattr(self, "permute" + str(i))
                t.inv_permutation = getattr(self, "inv_permute" + str(i))
                i += 1
            elif not isinstance(t, nn.Module):
                move_all_tensor_to_device(t, device)

        self.distribution.clear_cache()

        return self

    def forward_embedding(self, x: Tensor) -> Tensor:
        """Forward the embedded inputs i.e. the learned summary statistics.

        Args:
            x (Tensor): Inputs

        Returns:
            Tensor: Embedding
        """
        return self.embedding_net(x)

    def forward(self, x: Tensor) -> Distribution:
        """Returns a distribution given an input.

        Args:
            x (Tensor): Input

        Returns:
            Distribution: Output distribution
        """
        # Some issues with certain batch shapes
        expand = False

        if x.shape == torch.Size([]):
            x = x.reshape(1, 1)

        if x.shape.numel() > 1 and x.shape[0] == 1:
            x = x.squeeze(0)
            expand = True

        h = self.embedding_net(x)
        q = self.distribution.condition(h)

        if expand:
            q = q.expand((1,) + q.batch_shape)

        for t in q.transforms:
            if not isinstance(t, IndependentTransform):
                t._cache_size = int(self.with_cache)  # type: ignore

        return q
